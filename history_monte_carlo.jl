include("setup.jl")

struct HistoryMonteCarloTreeSearch
    ğ’«
    N
    Q
    d
    m
    c
    U
end

function explore(Ï€::HistoryMonteCarloTreeSearch,s, h)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ = possible_action(s,ğ’œ,R)
    Nh = sum(get(N, (h,a), 0) for a in ğ’œ)
    # print("state:",s,"\n")
    # print("ğ’œ:",ğ’œ,"\n")
    # print("argmax:",argmax(a->Q[(h,a)]+c*bonus(N[(h,a)],Nh),ğ’œ),"\n")
    # print("history:",h,"\n")
    return argmax(a->Q[(h,a)]+c*bonus(N[(h,a)],Nh),ğ’œ)
end

function simulate(Ï€::HistoryMonteCarloTreeSearch,s,h,d)
    if d <= 0
        return 0.0
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’®, ğ’œ, TRO, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.TRO, ğ’«.Î³
    # ğ’œ = possible_action(s,ğ’œ,R)
    if !haskey(N, (h,first(ğ’œ)))
        for a in ğ’œ
            N[(h,a)] = 0
            Q[(h,a)] = 0.0
        end
        return rollout(ğ’«,s,Ï€,d)
    end
    a = explore(Ï€, s, h)
    sâ€², r, o = TRO(s,a)
    q = r+Î³*simulate(Ï€,sâ€²,vcat(h,(a,o)), d-1)
    N[(h,a)] += 1
    Q[(h,a)] += (q-Q[(h,a)])/N[(h,a)]
    return q
end

bonus(Nsa,Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)

function (Ï€::HistoryMonteCarloTreeSearch)(b,ğ’œ,h=[])
    s = 0
    for i in 1:Ï€.m
        s = sample(Ï€.ğ’«.ğ’®,Weights(b))
        # s = rand(SetCategorical(Ï€.ğ’«.ğ’®,b))
        simulate(Ï€,s,h,Ï€.d)
    end
    ğ’œ = possible_action(argmax(b),ğ’œ,R)
    action = argmax(a->Ï€.Q[(h,a)],ğ’œ)
    print(Ï€.Q[(h,action)])
    return action
end

function rollout(ğ’«,s,Ï€,d)
    if d â‰¤ 0
        return 0.0
    end
    a = rollout_policy(s,ğ’«.ğ’œ)
    sâ€², r, o = TRO(s,a)
    return r + ğ’«.Î³*rollout(ğ’«,sâ€²,Ï€,d-1)
end

function rollout_policy(s,ğ’œ)
    ğ’œ = possible_action(s,ğ’œ,R)
    return sample(ğ’œ,Weights(ones(length(ğ’œ))))
end

type = MonteCarlo()
ğ’ª = copy(ğ’®)
fire, fire_all, reward_matrix = remove_fire(7543,fire,fire_all,reward_matrix)
fire, fire_all, reward_matrix = remove_fire(3137,fire,fire_all,reward_matrix)
R(s,a) = reward_matrix[s]
T(s,a,sâ€²) = trans_prob(type,dim,fire,s,a,sâ€²)
O(s) = observation_func(s)
TRO(s,a) = trans_reward_obs(type,ğ’œ,dim,fire,fire_all,s,a)
ğ’« = POMDP(Î³,ğ’®,ğ’œ,ğ’ª,T,R,O,TRO)
N = Dict{Tuple{Vector{Any},Int64},Int64}()
Q = Dict{Tuple{Vector{Any},Int64},Float64}()
# N[([],3)] = 0
# Q[([],3)] = 0.0
d = 60
m = 500
c = 2
Ï€ = HistoryMonteCarloTreeSearch(ğ’«,N,Q,d,m,c,nothing)
function propagate()
    s = 2001
    i = 1
    path1 = [s]
    while !any(s .== fire_all)
        b = O(s)
        a = Ï€(b,ğ’œ)
        s, _ = trans_reward(type,ğ’œ,dim,fire,fire_all,s,a)
        append!(path1,s)
        i += 1
        print(i,",",a,",",s,"\n")
        # if i == 2 break end
    end
    return path1
end
path1 = propagate()